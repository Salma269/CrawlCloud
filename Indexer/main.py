salma_sayed
salma_sayed
Sharing their screen

salma_sayed — 12/28/2024 7:13 PM
Image
Image
Image
Image
Image
Image
Image
Image
salma_sayed — 12/28/2024 7:22 PM
Image
Image
salma_sayed — 12/28/2024 7:55 PM
Image
Image
salma_sayed — 12/28/2024 10:14 PM
Here’s a structure for presentation slides on your authentication system. I’ll break it down by slide content:

---

### Slide 1: **Title Slide**
- **Title:** Secure Authentication System Using Nonces
Expand
message.txt
4 KB
Here is a suggested outline for creating slides about the provided code. The slides can be structured as follows:

---

### **Slide 1: Title Slide**
- **Title**: Optimized HTTP/2 Server in Python
Expand
message.txt
5 KB
FarahAhmed — 4/11/2025 9:56 PM
Forwarded
https://www.free-css.com/free-css-templates/page287/beautiflie
Beautiflie Free Website Template | Free CSS Templates | Free CSS
Preview of the Beautiflie Free CSS Template from HTML Design
IP project  •  4/11/2025
FarahAhmed — 4/11/2025 11:02 PM
https://chatgpt.com/share/67f9834b-b760-8000-bd53-b5fb85374e8d
ChatGPT
ChatGPT - Distributed Web Crawler Design
Shared via ChatGPT
Image
https://github.com/MuhammadMoeedUllah/webCrawler
GitHub
GitHub - MuhammadMoeedUllah/webCrawler: A crawler application based...
A crawler application based on aws cloud services. Any user can post a url link and then fetch the crawled webPage - MuhammadMoeedUllah/webCrawler
A crawler application based on aws cloud services. Any user can post a url link and then fetch the crawled webPage - MuhammadMoeedUllah/webCrawler
salma_sayed — 4/14/2025 10:01 PM
https://www.overleaf.com/latex/templates/ieee-conference-template/grfzhhncsfqn
salma_sayed — 4/17/2025 9:50 PM
Repo:
cv
embedded
database
contact list 
FarahAhmed — 4/17/2025 9:56 PM
https://drive.google.com/drive/folders/1IQlgXeZlgG4QDJAXtezmUFoLHpWajW-3?usp=drive_link
Google Drive
FarahAhmed — 4/17/2025 10:15 PM
https://github.com/Salma269/Barcode_Detector.git
FarahAhmed — 4/17/2025 10:23 PM
A computer vision pipeline that processes input images to detect, isolate, and decode barcodes using the Code 11 symbology. It applies preprocessing steps like grayscale conversion, noise reduction, gradient analysis, and morphological operations to locate the barcode region, then extracts the barcode signal and decodes it using a custom Code 11 implementation.
https://github.com/FarahElbosiley/AirlineDataBase.git
GitHub
GitHub - FarahElbosiley/AirlineDataBase: A Python-based application...
A Python-based application that manages an airline database with a graphical user interface. It allows users to view, add, update, and delete flight, passenger, and booking information. - FarahElbo...
GitHub - FarahElbosiley/AirlineDataBase: A Python-based application...
FarahAhmed — 4/17/2025 10:32 PM
https://github.com/Abdelrahman976/ATM-EDA-Project.git
GitHub
GitHub - Abdelrahman976/ATM-EDA-Project
Contribute to Abdelrahman976/ATM-EDA-Project development by creating an account on GitHub.
FarahAhmed — 4/17/2025 11:05 PM
https://github.com/FarahElbosiley/ContactList.git
GitHub
GitHub - FarahElbosiley/ContactList: A C++ contact list application...
A C++ contact list application that uses a Binary Search Tree (BST) for efficient storage and retrieval of contact information. The project features a modern GUI built with Qt, allowing users to ad...
A C++ contact list application that uses a Binary Search Tree (BST) for efficient storage and retrieval of contact information. The project features a modern GUI built with Qt, allowing users to ad...
https://github.com/FarahElbosiley/HospitalTesting.git
GitHub
GitHub - FarahElbosiley/HospitalTesting: Hospital Management system...
Hospital Management system with junit tests. Contribute to FarahElbosiley/HospitalTesting development by creating an account on GitHub.
Hospital Management system with junit tests. Contribute to FarahElbosiley/HospitalTesting development by creating an account on GitHub.
FarahAhmed — 4/17/2025 11:15 PM
https://github.com/eslam7-star/cms.git
GitHub
GitHub - eslam7-star/cms: Online Clinic Appointment Booking System
Online Clinic Appointment Booking System. Contribute to eslam7-star/cms development by creating an account on GitHub.
GitHub - eslam7-star/cms: Online Clinic Appointment Booking System
FarahAhmed — 4/17/2025 11:28 PM
BRAVO CLINIC MANAGEMENT SYSTEM | HTML,CSS, JAVASCRIPT, DJANGO May 2023
Ain Shams University
· Utilized the Django framework to build the backend of the web application successfully.
· Involved in the design process, requirements analysis, and modeling, including use-case diagrams, swim-lane diagrams, sequence diagrams, and class diagrams.
· Applied Agile development methodology to ensure efficient project management and iterative progress.
· Deployed and hosted the web application on https://eslam.pythonanywhere.com/
FarahAhmed — 4/18/2025 12:59 AM
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Task Manager</title>
Expand
Assignment.html
5 KB
salma_sayed — 4/18/2025 1:04 AM
const products = [
       { title: "Beauty Brush", description: "A premium makeup brush", image: "images/img-1.png", price: "$30" },
       { title: "Hair Comb", description: "Smooth and sleek comb", image: "images/img-2.png", price: "$25" },
       { title: "Makeup Kit", description: "Complete cosmetic kit", image: "images/img-3.png", price: "$50" },
       { title: "Nail Polish", description: "Glossy finish", image: "images/img-4.png", price: "$10" },
       { title: "Face Cream", description: "Moisturizing cream", image: "images/img-5.png", price: "$40" },
       { title: "Perfume", description: "Long-lasting fragrance", image: "images/img-6.png", price: "$60" },
       { title: "Lipstick", description: "Vibrant shades", image: "images/img-7.png", price: "$15" },
       { title: "Eyeliner", description: "Smooth application", image: "images/img-8.png", price: "$18" },
       { title: "Blush", description: "Natural color", image: "images/img-9.png", price: "$22" },
       { title: "Foundation", description: "Even skin tone", image: "images/img-10.png", price: "$45" },
       { title: "Concealer", description: "Covers imperfections", image: "images/img-11.png", price: "$35" },
       { title: "Highlighter", description: "Radiant glow", image: "images/img-12.png", price: "$28" },
    ];
FarahAhmed — 4/19/2025 12:13 AM
Image
Image
FarahAhmed — 4/19/2025 12:38 AM
Image
Crawler node send Newly discovered URLs back to the Master Node for potential future crawling.
All components send periodic updates to the Master Node. This allows monitoring of crawling progress, indexing status, and worker node health
 
Image
salma_sayed — 4/19/2025 1:02 AM
Image
Image
https://lucid.app/lucidchart/650cee48-926b-4e6c-ad92-fcccd3cb56c0/edit?viewport_loc=-1815%2C-201%2C2524%2C1168%2CmXwzAnnOLpm1&invitationId=inv_84e2cd2e-3d30-46fe-8620-94322aa48280
salma_sayed — 4/19/2025 1:27 AM
Image
salma_sayed — 4/19/2025 1:36 AM
Image
salma_sayed — 4/23/2025 1:12 AM
Image
FarahAhmed — 4/23/2025 1:24 AM
ps aux | grep redis
FarahAhmed — 5/3/2025 1:30 AM
from mpi4py import MPI
import subprocess
import socket
import traceback
import logging

logging.basicConfig(level=logging.INFO, format="%(asctime)s - Indexer - %(levelname)s - %(message)s")

def get_local_ip():
    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    try:
        s.connect(('8.8.8.8', 80))
        ip = s.getsockname()[0]
    except Exception:
        ip = '127.0.0.1'
    finally:
        s.close()
    return ip


ROLE_MAP = {
    '172.31.80.40': 'master',
    '172.31.80.247': 'crawler',
    '172.31.21.53': 'indexer'
}

comm = MPI.COMM_WORLD
rank = comm.Get_rank()

local_ip = get_local_ip()
role = ROLE_MAP.get(local_ip, 'unknown')

if rank == 0:
    from masterscript import master_process
    master_process()

elif role == 'crawler':
    try:
        command = "bash -c 'cd webcrawler && scrapy crawl mycrawler'"
        subprocess.run(command, shell=True, check=True)
        comm.send(f"[{local_ip}] Crawler finished successfully.", dest=0, tag=11)
    except Exception as e:
        error_msg = f"[{local_ip}] Crawler failed:\n{traceback.format_exc()}"
        comm.send(error_msg, dest=0, tag=99)

elif role == 'indexer':
    try:
        from indexer import indexer_process
        logging.info("sssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssStarting spider run")
        indexer_process()
        logging.info("Staxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxrting spider run")
        comm.send(f"[{local_ip}] Indexer finished successfully.", dest=0, tag=12)
    except Exception as e:
        logging.info(f"[{local_ip}] Indexer failed:\n{traceback.format_exc()}")
        error_msg = f"[{local_ip}] Indexer failed:\n{traceback.format_exc()}"
        comm.send(error_msg, dest=0, tag=98)

else:
    error_msg = f"[Rank {rank}] Unknown role for IP: {local_ip}"
    comm.send(error_msg, dest=0, tag=97)
salma_sayed — 5/5/2025 3:49 AM
\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{algorithmic}
\usepackage{amsmath}
Expand
message.txt
9 KB
The project report should have three parts: 
Relevance: a short (one sentence) summary of how your project relates to the class.
Abstract: Short summary describing the idea of the project (two sentences)
Content:
 Introduction: A general description of the area of your project and why you're 
doing it. 
 Problem Specification: A clear and brief technical description of the problem 
you're addressing.
 Approach: A description of how you went about trying to solve the problem. 
A short readme describing the implemented machines, the used programming 
language, the accepted language, etc.
 Results and Analysis: What happened when you evaluated your system. 
Screen shots of the outputs of the program 
 Conclusions: What did you learn from doing the project? What did you 
demonstrate about how to solve your problem?
 References: Complete list of sources you used in completing your project, 
with explanations of what you got from each.
salma_sayed — 5/5/2025 4:16 AM
Image
FarahAhmed — 5/5/2025 4:41 AM
Scrollable frame for pickup buttons
        pickup_canvas = tk.Canvas(self.pickup_frame, bg=self.card_color, 
                                 highlightthickness=0)
        scrollbar = ttk.Scrollbar(self.pickup_frame, orient="vertical", 
                                 command=pickup_canvas.yview)
        scrollable_frame = tk.Frame(pickup_canvas, bg=self.card_color)

        scrollable_frame.bind(
            "<Configure>",
            lambda e: pickup_canvas.configure(
                scrollregion=pickup_canvas.bbox("all")
            )
        )
salma_sayed — 5/5/2025 4:53 AM
Image
Image
Image
Image
Image
Image
Image
salma_sayed — 5/5/2025 5:00 AM
Image
salma_sayed — 5/6/2025 2:38 AM
fef4ec
salma_sayed — 5/8/2025 2:36 AM
https://drive.google.com/drive/folders/1y8lARWBQC9FB9ReFGMgSfw2nCzLTuJgq
Google Drive: Sign-in
Access Google Drive with a Google account (for personal use) or Google Workspace account (for business use).
FarahAhmed — 5/8/2025 2:40 AM
Image
salma_sayed — 2:35 AM
import boto3
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor
from scrapy import signals
from bs4 import BeautifulSoup
import json
Expand
message.txt
9 KB
ubuntu@ip-172-31-80-40:~$ python3 launcher.py
[Launcher] Reading nodes.txt...
[Launcher] Checking SSH access...
[Launcher] Reachable nodes: ['172.31.80.247', '172.31.21.53']
[Launcher] Launching mpirun with 3 processes...
[1,0]<stdout>:[Rank 0] Local IP: 172.31.80.40, Role: master
Expand
message.txt
11 KB
from flask import Flask, request, render_template_string
from collections import defaultdict
from mpi4py import MPI
import time
import logging
import boto3
Expand
message.txt
8 KB
FarahAhmed — 2:42 AM
# Add these at the VERY TOP of the file
import mpi4py
mpi4py.rc.initialize = False  # Disable auto-initialization
from mpi4py import MPI

# Initialize MPI once for all roles
Expand
message.txt
6 KB
from mpi4py import MPI
import subprocess
import socket
import traceback
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

def get_local_ip():
    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    try:
        s.connect(('8.8.8.8', 80))
        ip = s.getsockname()[0]
    except Exception:
        ip = '127.0.0.1'
    finally:
        s.close()
    return ip

# IP-to-role mapping
ROLE_MAP = {
    '172.31.80.40': 'master',
    '172.31.80.247': 'crawler',
    '172.31.21.53': 'indexer'
}

# Initialize MPI
comm = MPI.COMM_WORLD
rank = comm.Get_rank()

# Identify local IP and role
local_ip = get_local_ip()
role = ROLE_MAP.get(local_ip, 'unknown')

print(f"[Rank {rank}] Local IP: {local_ip}, Role: {role}")
logging.info(f"[Rank {rank}] Local IP: {local_ip}, Role: {role}")

# Master process
if rank == 0:
    print(f"[Rank {rank}] Running master process...")
    try:
        from masterscript import master_process
        master_process()
        print("[Master] Finished master_process")
    except Exception as e:
        error_msg = f"[{local_ip}] Master failed:\n{traceback.format_exc()}"
        logging.error(error_msg)
        comm.send(error_msg, dest=0, tag=97)

# Crawler node
elif role == 'crawler':
    print(f"[Rank {rank}] Crawler node starting subprocess...")
    try:
        command = "bash -c 'cd webcrawler && scrapy crawl mycrawler'"
        subprocess.run(command, shell=True, check=True, timeout=300)
        print(f"[Crawler] Finished crawling.")
        comm.send(f"[{local_ip}] Crawler finished successfully.", dest=0, tag=11)
    except Exception as e:
        error_msg = f"[{local_ip}] Crawler failed:\n{traceback.format_exc()}"
        logging.error(error_msg)
        comm.send(error_msg, dest=0, tag=99)

# Indexer node
elif role == 'indexer':
    print(f"[Rank {rank}] Indexer node starting...")
    try:
        from indexer import indexer_process
        logging.info("Starting indexer_process...")
        indexer_process()
        logging.info("Indexer process completed.")
        comm.send(f"[{local_ip}] Indexer finished successfully.", dest=0, tag=12)
    except Exception as e:
        error_msg = f"[{local_ip}] Indexer failed:\n{traceback.format_exc()}"
        logging.error(error_msg)
        comm.send(error_msg, dest=0, tag=98)

# Unknown role
else:
    print(f"[Rank {rank}] Unknown role for IP {local_ip}")
    error_msg = f"[Rank {rank}] Unknown role for IP: {local_ip}"
    logging.error(error_msg)
    comm.send(error_msg, dest=0, tag=97)
Collapse
message.txt
3 KB
FarahAhmed — 2:50 AM
from mpi4py import MPI
import subprocess
import socket
import traceback
import logging
Expand
message.txt
3 KB
﻿
# Add these at the VERY TOP of the file
import mpi4py
mpi4py.rc.initialize = False  # Disable auto-initialization
from mpi4py import MPI

# Initialize MPI once for all roles
required_thread_level = MPI.THREAD_MULTIPLE
current_level = MPI.Init_thread(required_thread_level)
if current_level < required_thread_level:
    raise RuntimeError(f"MPI_THREAD_MULTIPLE not supported (level {current_level})")

comm = MPI.COMM_WORLD
status = MPI.Status()
rank = comm.Get_rank()

# Rest of your existing imports
import subprocess
import socket
import traceback
import logging
from twisted.internet import reactor, defer, task
from scrapy.crawler import CrawlerRunner
from scrapy.utils.project import get_project_settings
from scrapy.utils.log import configure_logging
import threading
import time

# Spider import remains the same
from webcrawler.webcrawler.spiders.mycrawler import CrawlingSpider

logging.basicConfig(level=logging.INFO, format="%(asctime)s - Indexer - %(levelname)s - %(message)s")

def get_local_ip():
    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    try:
        s.connect(('8.8.8.8', 80))
        ip = s.getsockname()[0]
    except Exception:
        ip = '127.0.0.1'
    finally:
        s.close()
    return ip

ROLE_MAP = {
    '172.31.80.40': 'master',
    '172.31.80.247': 'crawler',
    '172.31.21.53': 'indexer',
    '172.31.30.16': 'client' 
}

local_ip = get_local_ip()
role = ROLE_MAP.get(local_ip, 'unknown')

if role == 'master' and rank == 0:
    from masterscript import master_process
    master_process()

elif role == 'crawler':
    # Configure Scrapy logging
    configure_logging()
    
    # Initialize crawler system
    settings = get_project_settings()
    runner = CrawlerRunner(settings)
    crawling_in_progress = False
    crawl_lock = threading.Lock()

    def trigger_listener():
        """Dedicated thread for handling crawl triggers"""
        while True:
            try:
                # Blocking receive for crawl triggers (tag 42)
                if comm.Iprobe(source=0, tag=42, status=status):
                    trigger = comm.recv(source=0, tag=42)
                    logging.info(f"Received crawl trigger: {trigger}")

                    with crawl_lock:
                        if not crawling_in_progress:
                            # Schedule in reactor thread
                            pass
                            reactor.callFromThread(start_crawl, trigger)
                        else:
                            comm.send(f"[{local_ip}] Crawler busy, skipping trigger", 
                                    dest=0, tag=96)

            except Exception as e:
                error_msg = f"[{local_ip}] Trigger Error:\n{traceback.format_exc()}"
                comm.send(error_msg, dest=0, tag=99)

    def ping_listener():
        """Dedicated thread for handling pings"""
        c=0
        while True:
            try:
                if comm.Iprobe(source=0, tag=100, status=status):
                    logging.info("xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx")
                    c+=1
#                    time.sleep(5)
                    # Blocking receive for pings (tag 100)
                    message = comm.recv(source=0, tag=100)
                    if message == "ping":
#                        if c < 1:
                        comm.send("pong", dest=0, tag=101)
                        logging.info(f"Responded to ping {c} from master")
            except Exception as e:
                error_msg = f"[{local_ip}] Ping Error:\n{traceback.format_exc()}"
                comm.send(error_msg, dest=0, tag=99)

    def start_crawl(trigger):
        """Start the spider run (called in reactor thread)"""
        global crawling_in_progress
        with crawl_lock:
            crawling_in_progress = True
        logging.info("Starting spider run")

        deferred = runner.crawl(CrawlingSpider, trigger_data=trigger)
        deferred.addCallbacks(crawl_success, crawl_error)

    def crawl_success(result):
        """Handle successful crawl completion"""
        global crawling_in_progress
        with crawl_lock:
            crawling_in_progress = False
        comm.send(f"[{local_ip}] Crawler finished successfully", dest=0, tag=11)
        logging.info("Spider finished successfully")
        return result

    def crawl_error(failure):
        """Handle crawl failures"""
        global crawling_in_progress
        with crawl_lock:
            crawling_in_progress = False
        error_msg = f"[{local_ip}] Crawler failed:\n{failure.getTraceback()}"
        comm.send(error_msg, dest=0, tag=99)
        logging.error("Spider failed: %s", failure.getErrorMessage())
        return failure

    # Start dedicated listener threads
    threading.Thread(target=ping_listener, daemon=True).start()
    threading.Thread(target=trigger_listener, daemon=True).start()
    
    # Start reactor (main thread)
    reactor.run()

elif role == 'indexer':
    try:
        from indexer import indexer_process
        indexer_process()
        comm.send(f"[{local_ip}] Indexer finished successfully.", dest=0, tag=12)
    except Exception as e:
        error_msg = f"[{local_ip}] Indexer failed:\n{traceback.format_exc()}"
        comm.send(error_msg, dest=0, tag=98)

elif role == 'client':
    try:
        from client import client_process
        client_process()

    except Exception as e:
        error_msg = f"[{local_ip}] Client failed:\n{traceback.format_exc()}"
        comm.send(error_msg, dest=0, tag=98)

else:
    error_msg = f"[Rank {rank}] Unknown role for IP: {local_ip}"
    comm.send(error_msg, dest=0, tag=97)
